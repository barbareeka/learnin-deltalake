{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this video :\n",
    "https://www.youtube.com/watch?v=u1VfOiHVeMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "findspark.init(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DeltaLake Transaction Logs\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '4')\n",
    "spark.conf.set('spark.default.parallelism', '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load helper functions\n",
    "\n",
    "%run Helpers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted path /tmp/spark/data/delta/online_retail_data\n",
      "Deleted path /tmp/spark/data/parquet/online_retail_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Source data\n",
    "# Change paths to match your environment!\n",
    "\n",
    "inputPath    = \"/tmp/spark/data/source/\"\n",
    "sourceData   = inputPath + \"online-retail-dataset.csv\"\n",
    "\n",
    "# Base location for all saved data\n",
    "basePath     = \"/tmp/spark/data\" \n",
    "\n",
    "# Path for Parquet formatted data\n",
    "parquetPath  = basePath + \"/parquet/online_retail_data\"\n",
    "\n",
    "# Path for Delta formatted data\n",
    "deltaPath    = basePath + \"/delta/online_retail_data\"\n",
    "deltaLogPath = deltaPath + \"/_delta_log\"\n",
    "\n",
    "# Clean up from last run.\n",
    "! rm -Rf $deltaPath 2>/dev/null\n",
    "print(\"Deleted path \" + deltaPath)\n",
    "\n",
    "! rm -Rf $parquetPath 2>/dev/null\n",
    "print(\"Deleted path \" + parquetPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Downloading dataset.\n",
      "-> Dataset is present.\n",
      "\n",
      "File [/tmp/spark/data/source/online-retail-dataset.csv] is ['44'] MB in size.\n"
     ]
    }
   ],
   "source": [
    "# Data sourced from \"Spark - The Definitive Guide\", located at: https://github.com/databricks/Spark-The-Definitive-Guide\n",
    "# Data origin: http://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
    "import os.path\n",
    "\n",
    "file_exists = os.path.isfile(f'{sourceData}')\n",
    " \n",
    "if not file_exists:\n",
    "    print(\"-> Downloading dataset.\")\n",
    "    os.system(f'mkdir -p {inputPath}')\n",
    "    os.system(f'curl https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv -o {sourceData}')\n",
    "    file_exists = os.path.isfile(f'{sourceData}')\n",
    "    \n",
    "if file_exists:\n",
    "    print(\"-> Dataset is present.\\n\")\n",
    "    \n",
    "    fileSize = ! du -m \"$sourceData\" | cut -f1 # Posix compliant\n",
    "    print(f\"File [{sourceData}] is {fileSize} MB in size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n",
      "536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n",
      "536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n",
      "536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n",
      "536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n"
     ]
    }
   ],
   "source": [
    "! head -5 {sourceData}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide schema for source data\n",
    "# Schema source: http://archive.ics.uci.edu/ml/datasets/Online+Retail#\n",
    "\n",
    "# SQL DDL method\n",
    "schemaDDL = \"\"\"InvoiceNo Integer, StockCode String, Description String, Quantity Integer, \n",
    "               InvoiceDate String, UnitPrice Double, CustomerID Integer, Country String \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count is 541909 and Partition cnt os 4\n"
     ]
    }
   ],
   "source": [
    "rawSalesDataDF = spark.read \\\n",
    ".schema(schemaDDL) \\\n",
    ".option(\"header\", True) \\\n",
    ".csv(sourceData)\n",
    "\n",
    "rwCnt = rawSalesDataDF.count()\n",
    "rwPart = rawSalesDataDF.rdd.getNumPartitions()\n",
    "\n",
    "print(f'Count is {rwCnt} and Partition cnt os {rwPart}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|     9291|        0|       1454|       0|          0|        0|    135080|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify Columns with null values\n",
    "\n",
    "rawSalesDataDF.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in rawSalesDataDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total count is 397924\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|          0|       0|          0|        0|         0|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanSalesDataDF = rawSalesDataDF.filter(\"InvoiceNo is not null and CustomerID is not null\")\n",
    "print(f\" Total count is {cleanSalesDataDF.count()}\")\n",
    "\n",
    "cleanSalesDataDF.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in cleanSalesDataDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: 99226 Partition Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Define new dataframe based on cleansed data but only use a subset of the data to make things run faster\n",
    "\n",
    "# Random sample of 25%, with seed and without replacement\n",
    "retailSalesData1 = cleanSalesDataDF.sample(withReplacement=False, fraction=.25, seed=75)\n",
    "\n",
    "# Count rows and partitions\n",
    "rowCount = retailSalesData1.count() \n",
    "partCount = retailSalesData1.rdd.getNumPartitions()\n",
    "\n",
    "print(f'Row Count: {rowCount} Partition Count: {partCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |12/1/2010 8:26|4.25     |17850     |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM          |6       |12/1/2010 8:34|2.1      |13047     |United Kingdom|\n",
      "|536367   |84969    |BOX OF 6 ASSORTED COLOUR TEASPOONS |6       |12/1/2010 8:34|4.25     |13047     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retailSalesData1.show(5, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|deltademo|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|         deltademo|\n",
      "+------------------+\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create database to hold demo objects\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS deltademo\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Current DB should be deltademo\n",
    "spark.sql(\"USE deltademo\")\n",
    "spark.sql(\"SELECT CURRENT_DATABASE()\").show()\n",
    "#spark.sql(\"DESCRIBE DATABASE deltademo\").show(truncate = False)\n",
    "\n",
    "# Clean-up from last run\n",
    "spark.sql(\"DROP TABLE IF EXISTS SalesParquetFormat\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS SalesDeltaFormat\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_CheckpointFile\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailSalesData1.write.saveAsTable('SalesParquetFormat', format='parquet', mode='overwrite', path=parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195KB    2020-09-26 10:09:11  part-00003-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "282KB    2020-09-26 10:09:11  part-00000-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "312KB    2020-09-26 10:09:11  part-00001-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "314KB    2020-09-26 10:09:11  part-00002-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "\n",
      "Number of file/s: 4 | Total size: 1.2M\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(parquetPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------+-------+\n",
      "|col_name                    |data_type                                      |comment|\n",
      "+----------------------------+-----------------------------------------------+-------+\n",
      "|InvoiceNo                   |int                                            |null   |\n",
      "|StockCode                   |string                                         |null   |\n",
      "|Description                 |string                                         |null   |\n",
      "|Quantity                    |int                                            |null   |\n",
      "|InvoiceDate                 |string                                         |null   |\n",
      "|UnitPrice                   |double                                         |null   |\n",
      "|CustomerID                  |int                                            |null   |\n",
      "|Country                     |string                                         |null   |\n",
      "|                            |                                               |       |\n",
      "|# Detailed Table Information|                                               |       |\n",
      "|Database                    |deltademo                                      |       |\n",
      "|Table                       |salesparquetformat                             |       |\n",
      "|Owner                       |vinyasshetty                                   |       |\n",
      "|Created Time                |Sat Sep 26 10:09:11 PDT 2020                   |       |\n",
      "|Last Access                 |UNKNOWN                                        |       |\n",
      "|Created By                  |Spark 3.0.1                                    |       |\n",
      "|Type                        |EXTERNAL                                       |       |\n",
      "|Provider                    |parquet                                        |       |\n",
      "|Statistics                  |1129731 bytes                                  |       |\n",
      "|Location                    |file:/tmp/spark/data/parquet/online_retail_data|       |\n",
      "+----------------------------+-----------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted SalesParquetFormat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                 |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "|563016   |21497    |FANCY FONTS BIRTHDAY WRAP   |25      |8/11/2011 12:44|0.42     |15358     |United Kingdom|\n",
      "|563016   |22993    |SET OF 4 PANTRY JELLY MOULDS|12      |8/11/2011 12:44|1.25     |15358     |United Kingdom|\n",
      "|563016   |22961    |JAM MAKING SET PRINTED      |12      |8/11/2011 12:44|1.45     |15358     |United Kingdom|\n",
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from SalesParquetFormat\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" insert into SalesParquetFormat \n",
    "              VALUES(963316, 2291, \"WORLD'S BEST JAM MAKING SET\", 5, \"08/13/2011 07:58\", 1.45, 15358, \"United Kingdom\")\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195KB    2020-09-26 10:09:11  part-00003-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "282KB    2020-09-26 10:09:11  part-00000-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "312KB    2020-09-26 10:09:11  part-00001-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "314KB    2020-09-26 10:09:11  part-00002-038c8e77-b750-4f8a-8240-7bbce1bacf37-c000.snappy.parquet\n",
      "2KB      2020-09-26 10:16:08  part-00000-98d508b8-040b-487f-9586-cd402154f430-c000.snappy.parquet\n",
      "\n",
      "Number of file/s: 5 | Total size: 1.2M\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(parquetPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                 |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "|563016   |21497    |FANCY FONTS BIRTHDAY WRAP   |25      |8/11/2011 12:44|0.42     |15358     |United Kingdom|\n",
      "|563016   |22993    |SET OF 4 PANTRY JELLY MOULDS|12      |8/11/2011 12:44|1.25     |15358     |United Kingdom|\n",
      "|563016   |22961    |JAM MAKING SET PRINTED      |12      |8/11/2011 12:44|1.45     |15358     |United Kingdom|\n",
      "+---------+---------+----------------------------+--------+---------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retailSalesData1.write.mode('overwrite').format('delta').save(deltaPath)\n",
    "\n",
    "# Query directly into the location with delta\n",
    "spark.sql(f\"select * from delta.`{deltaPath}` limit 3\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n",
      "|ver|timestamp          |operation|operationParameters                   |operationMetrics                                                  |\n",
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]|[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]|\n",
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2KB      2020-09-26 10:55:56  00000000000000000000.json\n",
      "\n",
      "Number of file/s: 1 | Total size: 4.0K\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath, 'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(add=None, commitInfo=Row(isBlindAppend=False, operation='WRITE', operationMetrics=Row(numFiles='4', numOutputBytes='1129731', numOutputRows='99226'), operationParameters=Row(mode='Overwrite', partitionBy='[]'), timestamp=1601142956522), metaData=None, protocol=None),\n",
       " Row(add=None, commitInfo=None, metaData=None, protocol=Row(minReaderVersion=1, minWriterVersion=2)),\n",
       " Row(add=None, commitInfo=None, metaData=Row(createdTime=1601142955491, format=Row(provider='parquet'), id='1cdd1401-0952-4573-acd4-69331ee931e3', partitionColumns=[], schemaString='{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNo\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StockCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Description\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Quantity\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceDate\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"UnitPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerID\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}'), protocol=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601142956000, path='part-00000-c031708e-6649-488a-ad2b-7b4988e9e4b4-c000.snappy.parquet', size=288465), commitInfo=None, metaData=None, protocol=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601142956000, path='part-00001-97510b30-dd26-479f-ab9e-a6372faa609d-c000.snappy.parquet', size=319754), commitInfo=None, metaData=None, protocol=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601142956000, path='part-00002-9e8e8b63-37c5-488d-b11d-6d17c0559893-c000.snappy.parquet', size=321712), commitInfo=None, metaData=None, protocol=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601142956000, path='part-00003-cb420873-82cb-4d22-b84a-480e369894ce-c000.snappy.parquet', size=199800), commitInfo=None, metaData=None, protocol=None)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load(deltaLogPath).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99149"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailSalesData2 = cleanSalesDataDF.sample(withReplacement=False, fraction=.25, seed=31)\n",
    "retailSalesData2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailSalesData2.write.mode('append').format('delta').save(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+-----------+\n",
      "|ver|timestamp          |operation|operationParameters                   |operationMetrics                                                  |readVersion|\n",
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+-----------+\n",
      "|1  |2020-09-26 11:01:54|WRITE    |[mode -> Append, partitionBy -> []]   |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]|0          |\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]|[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]|null       |\n",
      "+---+-------------------+---------+--------------------------------------+------------------------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Have to run this again , cannot directly use the previous history DF and expect to see updated info\n",
    "# See the readVersion , this will tell the order of transaction , we can use version column also\n",
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics','readVersion') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='salesparquetformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DROP TABLE IF EXISTS SalesDeltaFormat\n",
    "  \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE SalesDeltaFormat\n",
    "    USING DELTA\n",
    "    LOCATION '{}'\n",
    "  \"\"\".format(deltaPath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='salesdeltaformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='salesparquetformat', database='deltademo', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------+-------+\n",
      "|col_name                    |data_type                                    |comment|\n",
      "+----------------------------+---------------------------------------------+-------+\n",
      "|InvoiceNo                   |int                                          |       |\n",
      "|StockCode                   |string                                       |       |\n",
      "|Description                 |string                                       |       |\n",
      "|Quantity                    |int                                          |       |\n",
      "|InvoiceDate                 |string                                       |       |\n",
      "|UnitPrice                   |double                                       |       |\n",
      "|CustomerID                  |int                                          |       |\n",
      "|Country                     |string                                       |       |\n",
      "|                            |                                             |       |\n",
      "|# Partitioning              |                                             |       |\n",
      "|Not partitioned             |                                             |       |\n",
      "|                            |                                             |       |\n",
      "|# Detailed Table Information|                                             |       |\n",
      "|Name                        |deltademo.salesdeltaformat                   |       |\n",
      "|Location                    |file:/tmp/spark/data/delta/online_retail_data|       |\n",
      "|Provider                    |delta                                        |       |\n",
      "|Table Properties            |[]                                           |       |\n",
      "+----------------------------+---------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"desc formatted SalesDeltaFormat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Invoice # => 569522\n"
     ]
    }
   ],
   "source": [
    "# Let's find a Invoice with only 1 count and use it to test DML.\n",
    "oneRandomInvoice = spark.sql(\"\"\" SELECT InvoiceNo, count(*)\n",
    "                                 FROM SalesDeltaFormat\n",
    "                                 GROUP BY InvoiceNo\n",
    "                                 ORDER BY 2 asc\n",
    "                                 LIMIT 1\n",
    "                             \"\"\").collect()[0][0]\n",
    "\n",
    "print(f\"Random Invoice # => {oneRandomInvoice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+---------------+---------+----------+-------+\n",
      "|FileName                                                           |InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate    |UnitPrice|CustomerID|Country|\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+---------------+---------+----------+-------+\n",
      "|part-00002-bc283a61-7bc1-4f81-ab11-813ad73ae653-c000.snappy.parquet|569522   |84997D   |CHILDRENS CUTLERY POLKADOT PINK|72      |10/4/2011 14:41|3.75     |12664     |Finland|\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+---------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before DML (insert)\n",
    "# See the function input_file_name , tells from which file the record is coming from.Not a delta specific feature\n",
    "spark.sql(f\"\"\"\n",
    "              SELECT SUBSTRING(input_file_name(), -67, 67) AS FileName,\n",
    "                     * FROM SalesDeltaFormat \n",
    "              WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "               INSERT INTO SalesDeltaFormat\n",
    "               VALUES({oneRandomInvoice}, 2291, \"WORLD'S BEST JAM MAKING SET\", 5, \"08/13/2011 07:58\", 1.45, 15358, \"France\");\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2KB      2020-09-26 10:55:56  00000000000000000000.json\n",
      "938B     2020-09-26 11:01:54  00000000000000000001.json\n",
      "410B     2020-09-26 11:15:32  00000000000000000002.json\n",
      "\n",
      "Number of file/s: 3 | Total size: 12K\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(add=None, commitInfo=Row(isBlindAppend=True, operation='WRITE', operationMetrics=Row(numFiles='1', numOutputBytes='2369', numOutputRows='1'), operationParameters=Row(mode='Append', partitionBy='[]'), readVersion=1, timestamp=1601144132398)),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601144132000, path='part-00000-34347f0c-d248-425a-bcdf-a1d0f9ddf145-c000.snappy.parquet', size=2369), commitInfo=None)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF = spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000002.json\")\n",
    "#dfLog.printSchema()\n",
    "logDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|FileName                                                           |InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country|\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|part-00002-bc283a61-7bc1-4f81-ab11-813ad73ae653-c000.snappy.parquet|569522   |84997D   |CHILDRENS CUTLERY POLKADOT PINK|72      |10/4/2011 14:41 |3.75     |12664     |Finland|\n",
      "|part-00000-34347f0c-d248-425a-bcdf-a1d0f9ddf145-c000.snappy.parquet|569522   |2291     |WORLD'S BEST JAM MAKING SET    |5       |08/13/2011 07:58|1.45     |15358     |France |\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After DML (insert)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "              SELECT SUBSTRING(input_file_name(), -67, 67) AS FileName, *\n",
    "                     FROM SalesDeltaFormat \n",
    "                     WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update one invoice\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "              UPDATE SalesDeltaFormat\n",
    "              SET Quantity = Quantity + 1000\n",
    "              WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\")\n",
    "\n",
    "#deltaTable.update(\n",
    "#    condition=(\"InvoiceNo = oneRandomInvoice\"),\n",
    "#    set={\"Quantity\": expr(\"Quantity + 1000\")}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|FileName                                                           |InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country|\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|part-00000-6a1b9713-b947-44e7-ba46-1be65ce2df31-c000.snappy.parquet|569522   |84997D   |CHILDRENS CUTLERY POLKADOT PINK|1072    |10/4/2011 14:41 |3.75     |12664     |Finland|\n",
      "|part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet|569522   |2291     |WORLD'S BEST JAM MAKING SET    |1005    |08/13/2011 07:58|1.45     |15358     |France |\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After Update\n",
    "# Creates two new parquet file and one new transaction json file\n",
    "# Reads parquet file and writes it back with a new file with new data\n",
    "spark.sql(f\"\"\"\n",
    "              SELECT \n",
    "              SUBSTRING(input_file_name(), -67, 67) AS FileName, *\n",
    "              FROM SalesDeltaFormat \n",
    "              WHERE InvoiceNo = {oneRandomInvoice}\n",
    "           \"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "|ver|timestamp          |operation|operationParameters                     |operationMetrics                                                                       |readVersion|\n",
      "+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "|3  |2020-09-26 11:22:59|UPDATE   |[predicate -> (InvoiceNo#5861 = 569522)]|[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 29509]|2          |\n",
      "|2  |2020-09-26 11:15:32|WRITE    |[mode -> Append, partitionBy -> []]     |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |1          |\n",
      "|1  |2020-09-26 11:01:54|WRITE    |[mode -> Append, partitionBy -> []]     |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]                     |0          |\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]  |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                     |null       |\n",
      "+---+-------------------+---------+----------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics','readVersion') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195KB    2020-09-26 10:55:56  part-00003-cb420873-82cb-4d22-b84a-480e369894ce-c000.snappy.parquet\n",
      "282KB    2020-09-26 10:55:56  part-00000-c031708e-6649-488a-ad2b-7b4988e9e4b4-c000.snappy.parquet\n",
      "314KB    2020-09-26 10:55:56  part-00002-9e8e8b63-37c5-488d-b11d-6d17c0559893-c000.snappy.parquet\n",
      "312KB    2020-09-26 10:55:56  part-00001-97510b30-dd26-479f-ab9e-a6372faa609d-c000.snappy.parquet\n",
      "193KB    2020-09-26 11:01:54  part-00003-168600ad-ee98-43c9-90bf-a8edb8de8716-c000.snappy.parquet\n",
      "310KB    2020-09-26 11:01:54  part-00001-848f8231-6f8d-47c0-8599-3321cc8f720f-c000.snappy.parquet\n",
      "281KB    2020-09-26 11:01:54  part-00000-1218c889-b29d-4078-a213-f8fbfbfe7562-c000.snappy.parquet\n",
      "315KB    2020-09-26 11:01:54  part-00002-bc283a61-7bc1-4f81-ab11-813ad73ae653-c000.snappy.parquet\n",
      "2KB      2020-09-26 11:15:32  part-00000-34347f0c-d248-425a-bcdf-a1d0f9ddf145-c000.snappy.parquet\n",
      "2KB      2020-09-26 11:22:59  part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet\n",
      "315KB    2020-09-26 11:22:59  part-00000-6a1b9713-b947-44e7-ba46-1be65ce2df31-c000.snappy.parquet\n",
      "\n",
      "Number of file/s: 11 | Total size: 2.5M\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(deltaPath, \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2KB      2020-09-26 10:55:56  00000000000000000000.json\n",
      "938B     2020-09-26 11:01:54  00000000000000000001.json\n",
      "410B     2020-09-26 11:15:32  00000000000000000002.json\n",
      "902B     2020-09-26 11:22:59  00000000000000000003.json\n",
      "\n",
      "Number of file/s: 4 | Total size: 16K\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|FileName                                                           |InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country|\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "|part-00000-6a1b9713-b947-44e7-ba46-1be65ce2df31-c000.snappy.parquet|569522   |84997D   |CHILDRENS CUTLERY POLKADOT PINK|1072    |10/4/2011 14:41 |3.75     |12664     |Finland|\n",
      "|part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet|569522   |2291     |WORLD'S BEST JAM MAKING SET    |1005    |08/13/2011 07:58|1.45     |15358     |France |\n",
      "+-------------------------------------------------------------------+---------+---------+-------------------------------+--------+----------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before DML (delete)\n",
    "\n",
    "spark.sql(f\"\"\"select \n",
    "          substring(input_file_name(), -67, 67) as FileName,\n",
    "          * from SalesDeltaFormat \n",
    "          where InvoiceNo = {oneRandomInvoice}\"\"\").show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/delta-io/delta/blob/master/examples/python/quickstart.py\n",
    "# Delete and invoice (two records)\n",
    "\n",
    "# This results in one new file being created.  One file had just the one record so it does not have to be re-created\n",
    "# Each of the two records were in two different files. One of those files had only one record so it did not have to be re-created.\n",
    "\n",
    "spark.sql(f\"DELETE FROM SalesDeltaFormat WHERE InvoiceNo = {oneRandomInvoice}\")\n",
    "\n",
    "# deltaTable.delete(\n",
    "#    condition=(\"InvoiceNo = {537617}\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|FileName|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+--------+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# After DML (delete)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "              SELECT \n",
    "              SUBSTRING(input_file_name(), -67, 67) as FileName, *\n",
    "              FROM SalesDeltaFormat \n",
    "              WHERE InvoiceNo = {oneRandomInvoice}\n",
    "          \"\"\").show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "|ver|timestamp          |operation|operationParameters                                                               |operationMetrics                                                                       |readVersion|\n",
      "+---+-------------------+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "|4  |2020-09-26 11:36:54|DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 569522)\"]]|[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 29509]|3          |\n",
      "|3  |2020-09-26 11:22:59|UPDATE   |[predicate -> (InvoiceNo#5861 = 569522)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 29509]|2          |\n",
      "|2  |2020-09-26 11:15:32|WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                            |1          |\n",
      "|1  |2020-09-26 11:01:54|WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]                     |0          |\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]                                            |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                     |null       |\n",
      "+---+-------------------+---------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics','readVersion') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomy update 5 random invoices to force a checkpoint\n",
    "\n",
    "count = 0\n",
    "anInvoice = retailSalesData2.select(\"InvoiceNo\").orderBy(rand()).limit(1).collect()[0][0]\n",
    "\n",
    "while (count <= 5):\n",
    "  deltaTable.update(\n",
    "    condition=(f\"InvoiceNo = {anInvoice}\"),\n",
    "    set={\"Quantity\": expr(\"Quantity + 100\")})\n",
    "\n",
    "  count = count + 1\n",
    "  anInvoice = retailSalesData2.select(\"InvoiceNo\").orderBy(rand()).limit(1).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------+--------+----------+\n",
      "|txn |add                                                                                                      |remove                                                                                     |metaData|protocol|commitInfo|\n",
      "+----+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------+--------+----------+\n",
      "|null|null                                                                                                     |[part-00000-faebb022-ac41-4e64-b1f1-01155a0a1174-c000.snappy.parquet, 1601145781282, false]|null    |null    |null      |\n",
      "|null|null                                                                                                     |[part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet, 1601145413985, false]|null    |null    |null      |\n",
      "|null|null                                                                                                     |[part-00001-dd868f0f-7575-4ec2-b939-eb32ca0e5257-c000.snappy.parquet, 1601145781282, false]|null    |null    |null      |\n",
      "|null|[part-00001-5f1ab46d-7636-45c6-a328-7aa22bf32dae-c000.snappy.parquet, [], 321724, 1601145778000, false,,]|null                                                                                       |null    |null    |null      |\n",
      "|null|[part-00000-2dfb3f53-abc1-43aa-9a9c-0aa2b41b2ce9-c000.snappy.parquet, [], 321896, 1601145778000, false,,]|null                                                                                       |null    |null    |null      |\n",
      "|null|[part-00000-b8b8aacb-e4f3-43d6-b0af-5daf47c99e3b-c000.snappy.parquet, [], 288495, 1601145781000, false,,]|null                                                                                       |null    |null    |null      |\n",
      "|null|[part-00000-ce38c236-0e77-4fe2-b697-73f216ae38ff-c000.snappy.parquet, [], 319764, 1601145763000, false,,]|null                                                                                       |null    |null    |null      |\n",
      "|null|null                                                                                                     |[part-00000-c031708e-6649-488a-ad2b-7b4988e9e4b4-c000.snappy.parquet, 1601145766747, false]|null    |null    |null      |\n",
      "|null|[part-00001-4dfaf785-4911-43b8-9501-17d843aa89c1-c000.snappy.parquet, [], 317631, 1601145763000, false,,]|null                                                                                       |null    |null    |null      |\n",
      "|null|null                                                                                                     |[part-00001-97510b30-dd26-479f-ab9e-a6372faa609d-c000.snappy.parquet, 1601145762788, false]|null    |null    |null      |\n",
      "+----+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint after 10 transaction json file is created\n",
    "# Checkpoint parquetfile will usually have metadata to show lastest snapshot of the data , if you need time travle we will have to raed the json metadata file.\n",
    "\n",
    "checkPointDF = spark.read.format(\"parquet\").load(deltaLogPath + \"/00000000000000000010.checkpoint.parquet\")\n",
    "checkPointDF.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn: struct (nullable = true)\n",
      " |    |-- appId: string (nullable = true)\n",
      " |    |-- version: long (nullable = true)\n",
      " |    |-- lastUpdated: long (nullable = true)\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- partitionValues: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- stats: string (nullable = true)\n",
      " |    |-- tags: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- remove: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- deletionTimestamp: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |-- metaData: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- format: struct (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- options: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- schemaString: string (nullable = true)\n",
      " |    |-- partitionColumns: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- configuration: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- createdTime: long (nullable = true)\n",
      " |-- protocol: struct (nullable = true)\n",
      " |    |-- minReaderVersion: integer (nullable = true)\n",
      " |    |-- minWriterVersion: integer (nullable = true)\n",
      " |-- commitInfo: struct (nullable = true)\n",
      " |    |-- version: long (nullable = true)\n",
      " |    |-- timestamp: timestamp (nullable = true)\n",
      " |    |-- userId: string (nullable = true)\n",
      " |    |-- userName: string (nullable = true)\n",
      " |    |-- operation: string (nullable = true)\n",
      " |    |-- operationParameters: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- job: struct (nullable = true)\n",
      " |    |    |-- jobId: string (nullable = true)\n",
      " |    |    |-- jobName: string (nullable = true)\n",
      " |    |    |-- runId: string (nullable = true)\n",
      " |    |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |    |-- triggerType: string (nullable = true)\n",
      " |    |-- notebook: struct (nullable = true)\n",
      " |    |    |-- notebookId: string (nullable = true)\n",
      " |    |-- clusterId: string (nullable = true)\n",
      " |    |-- readVersion: long (nullable = true)\n",
      " |    |-- isolationLevel: string (nullable = true)\n",
      " |    |-- isBlindAppend: boolean (nullable = true)\n",
      " |    |-- operationMetrics: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- userMetadata: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkPointDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkPointFile10 =(\n",
    "    checkPointDF.select(col(\"add.path\").alias(\"FileAdded\"),\n",
    "                        col(\"add.modificationTime\").alias(\"DateAdded\"),\n",
    "                        col(\"remove.path\").alias(\"FileDeleted\"),\n",
    "                        col(\"remove.deletionTimestamp\").alias(\"DateDeleted\"))\n",
    "                .orderBy([\"DateAdded\",\"DateDeleted\"], ascending=[True,False])\n",
    ")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_checkpointfile\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS tbl_checkpointfile (Action string, filename string, ActionDate Long)\")\n",
    "\n",
    "checkPointFile10.createOrReplaceTempView(\"vw_checkpointfile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n",
      "|FileAdded                                                          |DateAdded    |FileDeleted                                                        |DateDeleted  |\n",
      "+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n",
      "|null                                                               |null         |part-00000-faebb022-ac41-4e64-b1f1-01155a0a1174-c000.snappy.parquet|1601145781282|\n",
      "|null                                                               |null         |part-00001-dd868f0f-7575-4ec2-b939-eb32ca0e5257-c000.snappy.parquet|1601145781282|\n",
      "|null                                                               |null         |part-00001-16bca056-f8f2-4a12-bc1b-b9201e57172a-c000.snappy.parquet|1601145777891|\n",
      "|null                                                               |null         |part-00000-969bc72f-8f05-4dbc-845d-d8509413634c-c000.snappy.parquet|1601145777891|\n",
      "|null                                                               |null         |part-00000-0363eae0-b480-4ff4-919d-6daa00cd5376-c000.snappy.parquet|1601145774052|\n",
      "|null                                                               |null         |part-00001-176aa90f-833d-4d92-a24d-196b086f8392-c000.snappy.parquet|1601145774052|\n",
      "|null                                                               |null         |part-00000-426b9079-97d8-40e3-8772-bcd9bcb45ad3-c000.snappy.parquet|1601145770263|\n",
      "|null                                                               |null         |part-00002-9e8e8b63-37c5-488d-b11d-6d17c0559893-c000.snappy.parquet|1601145770263|\n",
      "|null                                                               |null         |part-00000-c031708e-6649-488a-ad2b-7b4988e9e4b4-c000.snappy.parquet|1601145766747|\n",
      "|null                                                               |null         |part-00000-1218c889-b29d-4078-a213-f8fbfbfe7562-c000.snappy.parquet|1601145766747|\n",
      "|null                                                               |null         |part-00001-97510b30-dd26-479f-ab9e-a6372faa609d-c000.snappy.parquet|1601145762788|\n",
      "|null                                                               |null         |part-00001-848f8231-6f8d-47c0-8599-3321cc8f720f-c000.snappy.parquet|1601145762788|\n",
      "|null                                                               |null         |part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet|1601145413985|\n",
      "|null                                                               |null         |part-00000-6a1b9713-b947-44e7-ba46-1be65ce2df31-c000.snappy.parquet|1601145413985|\n",
      "|null                                                               |null         |part-00002-bc283a61-7bc1-4f81-ab11-813ad73ae653-c000.snappy.parquet|1601144578895|\n",
      "|null                                                               |null         |part-00000-34347f0c-d248-425a-bcdf-a1d0f9ddf145-c000.snappy.parquet|1601144578895|\n",
      "|null                                                               |null         |null                                                               |null         |\n",
      "|null                                                               |null         |null                                                               |null         |\n",
      "|part-00003-cb420873-82cb-4d22-b84a-480e369894ce-c000.snappy.parquet|1601142956000|null                                                               |null         |\n",
      "|part-00003-168600ad-ee98-43c9-90bf-a8edb8de8716-c000.snappy.parquet|1601143314000|null                                                               |null         |\n",
      "|part-00000-ce38c236-0e77-4fe2-b697-73f216ae38ff-c000.snappy.parquet|1601145763000|null                                                               |null         |\n",
      "|part-00001-4dfaf785-4911-43b8-9501-17d843aa89c1-c000.snappy.parquet|1601145763000|null                                                               |null         |\n",
      "|part-00001-5f1ab46d-7636-45c6-a328-7aa22bf32dae-c000.snappy.parquet|1601145778000|null                                                               |null         |\n",
      "|part-00000-2dfb3f53-abc1-43aa-9a9c-0aa2b41b2ce9-c000.snappy.parquet|1601145778000|null                                                               |null         |\n",
      "|part-00000-b8b8aacb-e4f3-43d6-b0af-5daf47c99e3b-c000.snappy.parquet|1601145781000|null                                                               |null         |\n",
      "|part-00001-3952f413-8067-4ea2-9a29-b8dafae1a2d1-c000.snappy.parquet|1601145781000|null                                                               |null         |\n",
      "+-------------------------------------------------------------------+-------------+-------------------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from vw_checkpointfile limit 100\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          INSERT INTO tbl_checkpointfile\n",
    "          SELECT \"Add\", FileAdded, DateAdded\n",
    "          FROM vw_checkpointfile\n",
    "          WHERE FileAdded IS NOT NULL\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          INSERT INTO tbl_checkpointfile\n",
    "          SELECT \"Delete\", FileDeleted, DateDeleted\n",
    "          FROM vw_checkpointfile\n",
    "          WHERE FileDeleted IS NOT NULL\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------+-------------------+\n",
      "|Action|File Name                                                          |ActionDate         |\n",
      "+------+-------------------------------------------------------------------+-------------------+\n",
      "|Add   |part-00003-cb420873-82cb-4d22-b84a-480e369894ce-c000.snappy.parquet|2020-09-26 10:55:56|\n",
      "|Add   |part-00003-168600ad-ee98-43c9-90bf-a8edb8de8716-c000.snappy.parquet|2020-09-26 11:01:54|\n",
      "|Delete|part-00002-bc283a61-7bc1-4f81-ab11-813ad73ae653-c000.snappy.parquet|2020-09-26 11:22:58|\n",
      "|Delete|part-00000-34347f0c-d248-425a-bcdf-a1d0f9ddf145-c000.snappy.parquet|2020-09-26 11:22:58|\n",
      "|Delete|part-00001-b3950e95-b902-47be-8203-792f0d9b62cc-c000.snappy.parquet|2020-09-26 11:36:53|\n",
      "|Delete|part-00000-6a1b9713-b947-44e7-ba46-1be65ce2df31-c000.snappy.parquet|2020-09-26 11:36:53|\n",
      "|Delete|part-00001-97510b30-dd26-479f-ab9e-a6372faa609d-c000.snappy.parquet|2020-09-26 11:42:42|\n",
      "|Delete|part-00001-848f8231-6f8d-47c0-8599-3321cc8f720f-c000.snappy.parquet|2020-09-26 11:42:42|\n",
      "|Add   |part-00000-ce38c236-0e77-4fe2-b697-73f216ae38ff-c000.snappy.parquet|2020-09-26 11:42:43|\n",
      "|Add   |part-00001-4dfaf785-4911-43b8-9501-17d843aa89c1-c000.snappy.parquet|2020-09-26 11:42:43|\n",
      "|Delete|part-00000-c031708e-6649-488a-ad2b-7b4988e9e4b4-c000.snappy.parquet|2020-09-26 11:42:46|\n",
      "|Delete|part-00000-1218c889-b29d-4078-a213-f8fbfbfe7562-c000.snappy.parquet|2020-09-26 11:42:46|\n",
      "|Delete|part-00000-426b9079-97d8-40e3-8772-bcd9bcb45ad3-c000.snappy.parquet|2020-09-26 11:42:50|\n",
      "|Delete|part-00002-9e8e8b63-37c5-488d-b11d-6d17c0559893-c000.snappy.parquet|2020-09-26 11:42:50|\n",
      "|Delete|part-00000-0363eae0-b480-4ff4-919d-6daa00cd5376-c000.snappy.parquet|2020-09-26 11:42:54|\n",
      "|Delete|part-00001-176aa90f-833d-4d92-a24d-196b086f8392-c000.snappy.parquet|2020-09-26 11:42:54|\n",
      "|Delete|part-00001-16bca056-f8f2-4a12-bc1b-b9201e57172a-c000.snappy.parquet|2020-09-26 11:42:57|\n",
      "|Delete|part-00000-969bc72f-8f05-4dbc-845d-d8509413634c-c000.snappy.parquet|2020-09-26 11:42:57|\n",
      "|Add   |part-00000-2dfb3f53-abc1-43aa-9a9c-0aa2b41b2ce9-c000.snappy.parquet|2020-09-26 11:42:58|\n",
      "|Add   |part-00001-5f1ab46d-7636-45c6-a328-7aa22bf32dae-c000.snappy.parquet|2020-09-26 11:42:58|\n",
      "|Delete|part-00000-faebb022-ac41-4e64-b1f1-01155a0a1174-c000.snappy.parquet|2020-09-26 11:43:01|\n",
      "|Delete|part-00001-dd868f0f-7575-4ec2-b939-eb32ca0e5257-c000.snappy.parquet|2020-09-26 11:43:01|\n",
      "|Add   |part-00000-b8b8aacb-e4f3-43d6-b0af-5daf47c99e3b-c000.snappy.parquet|2020-09-26 11:43:01|\n",
      "|Add   |part-00001-3952f413-8067-4ea2-9a29-b8dafae1a2d1-c000.snappy.parquet|2020-09-26 11:43:01|\n",
      "+------+-------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "           SELECT Action, \n",
    "                  filename as `File Name`, \n",
    "                  from_unixtime(actiondate/1e3) AS `ActionDate`\n",
    "           FROM tbl_checkpointfile \n",
    "           order by ActionDate asc\n",
    "          \"\"\").show(200, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------+\n",
      "|ver|timestamp          |operation|operationParameters                                                               |operationMetrics                                                                        |readVersion|\n",
      "+---+-------------------+---------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------+\n",
      "|10 |2020-09-26 11:43:01|UPDATE   |[predicate -> (InvoiceNo#3843 = 548808)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 20, numCopiedRows -> 50101]|9          |\n",
      "|9  |2020-09-26 11:42:58|UPDATE   |[predicate -> (InvoiceNo#3843 = 571184)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 11, numCopiedRows -> 58914]|8          |\n",
      "|8  |2020-09-26 11:42:54|UPDATE   |[predicate -> (InvoiceNo#3843 = 537374)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 50, numCopiedRows -> 50071]|7          |\n",
      "|7  |2020-09-26 11:42:50|UPDATE   |[predicate -> (InvoiceNo#3843 = 568670)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 17, numCopiedRows -> 58908]|6          |\n",
      "|6  |2020-09-26 11:42:47|UPDATE   |[predicate -> (InvoiceNo#3843 = 542618)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 29, numCopiedRows -> 50092]|5          |\n",
      "|5  |2020-09-26 11:42:43|UPDATE   |[predicate -> (InvoiceNo#3843 = 554814)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 21, numCopiedRows -> 54372]|4          |\n",
      "|4  |2020-09-26 11:36:54|DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 569522)\"]]|[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 29509] |3          |\n",
      "|3  |2020-09-26 11:22:59|UPDATE   |[predicate -> (InvoiceNo#5861 = 569522)]                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 29509] |2          |\n",
      "|2  |2020-09-26 11:15:32|WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                             |1          |\n",
      "|1  |2020-09-26 11:01:54|WRITE    |[mode -> Append, partitionBy -> []]                                               |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]                      |0          |\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]                                            |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                      |null       |\n",
      "+---+-------------------+---------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics','readVersion') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "history.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add some data to our table by doing a merge\n",
    "# Do this to show how json logs pick up after checkpoint\n",
    "\n",
    "# Create a tiny dataframe to use with merge\n",
    "mergeSalesData= cleanSalesDataDF.sample(withReplacement=False, fraction=.0001, seed=13)\n",
    "mergeSalesData.createOrReplaceTempView(\"vw_mergeSalesData\")\n",
    "\n",
    "# User-defined commit metadata\n",
    "# Usermedata of history table will get this info\n",
    "spark.sql(f\"\"\"\n",
    "               SET spark.databricks.delta.commitInfo.userMetadata=08-25-2020 Data Merge;\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO SalesDeltaFormat\n",
    "          USING vw_mergeSalesData\n",
    "          ON SalesDeltaFormat.StockCode = vw_mergeSalesData.StockCode\n",
    "           AND SalesDeltaFormat.InvoiceNo = vw_mergeSalesData.InvoiceNo\n",
    "          WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "          WHEN NOT MATCHED\n",
    "            THEN INSERT *\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|ver|timestamp          |operation|operationParameters                                                                                                                                                                               |operationMetrics                                                                                                                                                                                                         |readVersion|\n",
      "+---+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|11 |2020-09-26 11:59:58|MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|[numTargetRowsCopied -> 198349, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 26, numTargetRowsUpdated -> 25, numOutputRows -> 198400, numSourceRows -> 46, numTargetFilesRemoved -> 8]|10         |\n",
      "|10 |2020-09-26 11:43:01|UPDATE   |[predicate -> (InvoiceNo#3843 = 548808)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 20, numCopiedRows -> 50101]                                                                                                                                 |9          |\n",
      "|9  |2020-09-26 11:42:58|UPDATE   |[predicate -> (InvoiceNo#3843 = 571184)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 11, numCopiedRows -> 58914]                                                                                                                                 |8          |\n",
      "|8  |2020-09-26 11:42:54|UPDATE   |[predicate -> (InvoiceNo#3843 = 537374)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 50, numCopiedRows -> 50071]                                                                                                                                 |7          |\n",
      "|7  |2020-09-26 11:42:50|UPDATE   |[predicate -> (InvoiceNo#3843 = 568670)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 17, numCopiedRows -> 58908]                                                                                                                                 |6          |\n",
      "|6  |2020-09-26 11:42:47|UPDATE   |[predicate -> (InvoiceNo#3843 = 542618)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 29, numCopiedRows -> 50092]                                                                                                                                 |5          |\n",
      "|5  |2020-09-26 11:42:43|UPDATE   |[predicate -> (InvoiceNo#3843 = 554814)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 21, numCopiedRows -> 54372]                                                                                                                                 |4          |\n",
      "|4  |2020-09-26 11:36:54|DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 569522)\"]]                                                                                                                |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 29509]                                                                                                                                  |3          |\n",
      "|3  |2020-09-26 11:22:59|UPDATE   |[predicate -> (InvoiceNo#5861 = 569522)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 29509]                                                                                                                                  |2          |\n",
      "|2  |2020-09-26 11:15:32|WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                              |1          |\n",
      "|1  |2020-09-26 11:01:54|WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]                                                                                                                                                       |0          |\n",
      "|0  |2020-09-26 10:55:56|WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                                                                                                                                                       |null       |\n",
      "+---+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = deltaTable.history().select('version','timestamp','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics','readVersion') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "history.show(truncate=False)\n",
    "# I had not set spark.sql.shuffle.partitions to 4 while running the merge , so 200 files got created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|version|userMetadata          |\n",
      "+-------+----------------------+\n",
      "|11     |08-25-2020 Data Merge;|\n",
      "|10     |null                  |\n",
      "|9      |null                  |\n",
      "|8      |null                  |\n",
      "|7      |null                  |\n",
      "|6      |null                  |\n",
      "|5      |null                  |\n",
      "|4      |null                  |\n",
      "|3      |null                  |\n",
      "|2      |null                  |\n",
      "|1      |null                  |\n",
      "|0      |null                  |\n",
      "+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().select(\"version\",\"userMetadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ['     224'] files.\n"
     ]
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an artificial \"small file\" problem\n",
    "\n",
    "(spark.read\n",
    ".format(\"delta\")\n",
    ".load(deltaPath)\n",
    ".repartition(1000)\n",
    ".write\n",
    ".option(\"dataChange\", True)\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".save(deltaPath)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ['    1224'] files.\n"
     ]
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count => 4244\n",
      "\n",
      "CPU times: user 1.74 ms, sys: 2.12 ms, total: 3.86 ms\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spark.sql(\"select * from SalesDeltaFormat limit 2\").show()\n",
    "\n",
    "rowCount = spark.sql(\"\"\" SELECT CustomerID, count(Country) AS num_countries\n",
    "                         FROM SalesDeltaFormat\n",
    "                         GROUP BY CustomerID \n",
    "                     \"\"\").count()\n",
    "\n",
    "print(f\"Row Count => {rowCount}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact 1000 files to 4\n",
    "\n",
    "(spark.read\n",
    ".format(\"delta\")\n",
    ".load(deltaPath)\n",
    ".repartition(4)\n",
    ".write\n",
    ".option(\"dataChange\", False)\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ['    1228'] files.\n"
     ]
    }
   ],
   "source": [
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count => 4244\n",
      "CPU times: user 1.48 ms, sys: 1.89 ms, total: 3.37 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spark.sql(\"select * from SalesDeltaFormat limit 2\").show()\n",
    "rowCount = spark.sql(\"\"\" select CustomerID, count(Country) as num_countries\n",
    "                         from SalesDeltaFormat\n",
    "                        group by CustomerID \"\"\").count()\n",
    "\n",
    "print(f\"Row Count => {rowCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel Queries\n",
    "\n",
    "#POO currentVersion = deltaTable.history(1).select(\"version\").collect()[0][0]\n",
    "# Determine latest version of the Delta table\n",
    "currentVersion = spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat LIMIT 1\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: 198400 as of table version 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query table as of the current version to attain row count\n",
    "currentRowCount = spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(deltaPath).count()\n",
    "\n",
    "print(f\"Row Count: {currentRowCount} as of table version {currentVersion}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198400"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99174 more rows in version [13] than version [0] of the table.\n"
     ]
    }
   ],
   "source": [
    "# Determine difference in record count between the current version and the original version of the table.\n",
    "\n",
    "origRowCount = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath).count()\n",
    "print(f\"There are {currentRowCount-origRowCount} more rows in version [{currentVersion}] than version [0] of the table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll back current table to version 0 (original).\n",
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\",0)\n",
    "    .load(deltaPath)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current version should have same record count as version 0.\n",
    "\n",
    "currentVersion = spark.sql(\"DESCRIBE HISTORY SalesDeltaFormat LIMIT 1\").collect()[0][0]\n",
    "print(currentVersion)\n",
    "# If equal it will return \"true\"\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(deltaPath).count() == spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### HISTORY ########\n",
      "+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ver|operation|operationParameters                                                                                                                                                                               |operationMetrics                                                                                                                                                                                                         |\n",
      "+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|14 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                                                                                                                                                       |\n",
      "|13 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 2965071, numOutputRows -> 198400]                                                                                                                                                      |\n",
      "|12 |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 1000, numOutputBytes -> 11374483, numOutputRows -> 198400]                                                                                                                                                  |\n",
      "|11 |MERGE    |[predicate -> ((spark_catalog.deltademo.SalesDeltaFormat.`StockCode` = vw_mergesalesdata.`StockCode`) AND (spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = vw_mergesalesdata.`InvoiceNo`))]|[numTargetRowsCopied -> 198349, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 26, numTargetRowsUpdated -> 25, numOutputRows -> 198400, numSourceRows -> 46, numTargetFilesRemoved -> 8]|\n",
      "|10 |UPDATE   |[predicate -> (InvoiceNo#3843 = 548808)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 20, numCopiedRows -> 50101]                                                                                                                                 |\n",
      "|9  |UPDATE   |[predicate -> (InvoiceNo#3843 = 571184)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 11, numCopiedRows -> 58914]                                                                                                                                 |\n",
      "|8  |UPDATE   |[predicate -> (InvoiceNo#3843 = 537374)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 50, numCopiedRows -> 50071]                                                                                                                                 |\n",
      "|7  |UPDATE   |[predicate -> (InvoiceNo#3843 = 568670)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 17, numCopiedRows -> 58908]                                                                                                                                 |\n",
      "|6  |UPDATE   |[predicate -> (InvoiceNo#3843 = 542618)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 29, numCopiedRows -> 50092]                                                                                                                                 |\n",
      "|5  |UPDATE   |[predicate -> (InvoiceNo#3843 = 554814)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 21, numCopiedRows -> 54372]                                                                                                                                 |\n",
      "|4  |DELETE   |[predicate -> [\"(spark_catalog.deltademo.SalesDeltaFormat.`InvoiceNo` = 569522)\"]]                                                                                                                |[numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 29509]                                                                                                                                  |\n",
      "|3  |UPDATE   |[predicate -> (InvoiceNo#5861 = 569522)]                                                                                                                                                          |[numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 29509]                                                                                                                                  |\n",
      "|2  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 1, numOutputBytes -> 2369, numOutputRows -> 1]                                                                                                                                                              |\n",
      "|1  |WRITE    |[mode -> Append, partitionBy -> []]                                                                                                                                                               |[numFiles -> 4, numOutputBytes -> 1125004, numOutputRows -> 99149]                                                                                                                                                       |\n",
      "|0  |WRITE    |[mode -> Overwrite, partitionBy -> []]                                                                                                                                                            |[numFiles -> 4, numOutputBytes -> 1129731, numOutputRows -> 99226]                                                                                                                                                       |\n",
      "+---+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"####### HISTORY ########\")\n",
    "\n",
    "# Observe history of actions taken on a Delta table\n",
    "history = deltaTable.history().select('version','operation', 'operationParameters', \\\n",
    "                                      'operationMetrics') \\\n",
    "                              .withColumnRenamed(\"version\", \"ver\")\n",
    "\n",
    "history.show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Delta Vacuum - Data Retention\n",
    "\n",
    "delta.logRetentionDuration - default 30 days   \n",
    "delta.deletedFileRetentionDuration - default 30 days\n",
    "\n",
    "    Don't need to set them to be the same. You may want to keep the log files around after the tombstoned files are purged.\n",
    "    Time travel in order of months/years infeasible\n",
    "    Initially desinged to correct mistakes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ['    1232'] files.\n"
     ]
    }
   ],
   "source": [
    "# files_in_dir(deltaPath,\"parquet\")\n",
    "# Count files in deltaPath\n",
    "\n",
    "numFiles = ! ls $deltaPath/*parquet | wc -l\n",
    "print(f\"There are {numFiles} files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|path                                         |\n",
      "+---------------------------------------------+\n",
      "|file:/tmp/spark/data/delta/online_retail_data|\n",
      "+---------------------------------------------+\n",
      "\n",
      "CPU times: user 6.83 ms, sys: 5.18 ms, total: 12 ms\n",
      "Wall time: 59.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vacuum Delta table to remove all history\n",
    "\n",
    "spark.sql(\"VACUUM SalesDeltaFormat RETAIN 0 HOURS\").show(truncate = False)\n",
    "# ! Can use deltaTable.vacuum(0) against directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195KB    2020-09-26 12:23:40  part-00003-005548cc-1df0-4598-a4c8-f0d374761cc0-c000.snappy.parquet\n",
      "282KB    2020-09-26 12:23:40  part-00002-f24dfef8-ff99-4f73-980d-fcf7104acf95-c000.snappy.parquet\n",
      "314KB    2020-09-26 12:23:40  part-00000-865a0c57-3839-4392-82b2-bfc931f75c4f-c000.snappy.parquet\n",
      "312KB    2020-09-26 12:23:40  part-00001-ca10834b-9660-49de-bf41-9216be06a3ae-c000.snappy.parquet\n",
      "\n",
      "Number of file/s: 4 | Total size: 1.5M\n"
     ]
    }
   ],
   "source": [
    "files_in_dir(deltaPath,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2KB      2020-09-26 10:55:56  00000000000000000000.json\n",
      "938B     2020-09-26 11:01:54  00000000000000000001.json\n",
      "410B     2020-09-26 11:15:32  00000000000000000002.json\n",
      "902B     2020-09-26 11:22:59  00000000000000000003.json\n",
      "775B     2020-09-26 11:36:54  00000000000000000004.json\n",
      "905B     2020-09-26 11:42:43  00000000000000000005.json\n",
      "905B     2020-09-26 11:42:47  00000000000000000006.json\n",
      "905B     2020-09-26 11:42:50  00000000000000000007.json\n",
      "905B     2020-09-26 11:42:54  00000000000000000008.json\n",
      "905B     2020-09-26 11:42:58  00000000000000000009.json\n",
      "905B     2020-09-26 11:43:01  00000000000000000010.json\n",
      "17KB     2020-09-26 11:43:04  00000000000000000010.checkpoint.parquet\n",
      "35KB     2020-09-26 11:59:58  00000000000000000011.json\n",
      "196KB    2020-09-26 12:10:11  00000000000000000012.json\n",
      "141KB    2020-09-26 12:11:36  00000000000000000013.json\n",
      "2KB      2020-09-26 12:23:40  00000000000000000014.json\n",
      "\n",
      "Number of file/s: 16 | Total size: 452K\n"
     ]
    }
   ],
   "source": [
    "# So VACUUM DOES NOT CLEAN UP TRANSACTION LOG FILE, default is 30 days\n",
    "files_in_dir(deltaLogPath,\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(add=None, commitInfo=Row(isBlindAppend=False, operation='WRITE', operationMetrics=Row(numFiles='4', numOutputBytes='1129731', numOutputRows='99226'), operationParameters=Row(mode='Overwrite', partitionBy='[]'), readVersion=13, timestamp=1601148220630, userMetadata='08-25-2020 Data Merge;'), remove=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601148220000, path='part-00000-865a0c57-3839-4392-82b2-bfc931f75c4f-c000.snappy.parquet', size=321712), commitInfo=None, remove=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601148220000, path='part-00001-ca10834b-9660-49de-bf41-9216be06a3ae-c000.snappy.parquet', size=319754), commitInfo=None, remove=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601148220000, path='part-00002-f24dfef8-ff99-4f73-980d-fcf7104acf95-c000.snappy.parquet', size=288465), commitInfo=None, remove=None),\n",
       " Row(add=Row(dataChange=True, modificationTime=1601148220000, path='part-00003-005548cc-1df0-4598-a4c8-f0d374761cc0-c000.snappy.parquet', size=199800), commitInfo=None, remove=None),\n",
       " Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1601148220630, path='part-00003-e5185b45-5608-4f60-b9d2-6593e5961fa4-c000.snappy.parquet')),\n",
       " Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1601148220630, path='part-00002-d322b49c-bbba-4396-96cb-f300203029d3-c000.snappy.parquet')),\n",
       " Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1601148220630, path='part-00001-978cd473-c9a5-40a2-be22-be961825a2dc-c000.snappy.parquet')),\n",
       " Row(add=None, commitInfo=None, remove=Row(dataChange=True, deletionTimestamp=1601148220630, path='part-00000-ea334bc6-c0a4-4c7b-adf9-36bab5babbd6-c000.snappy.parquet'))]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(deltaLogPath + \"/00000000000000000014.json\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Delta table to keep around 7 days of deleted data and 7 days of older log files\n",
    "spark.sql(\"alter table SalesDeltaFormat set tblproperties ('delta.logRetentionDuration' = 'interval 7 days', 'delta.deletedFileRetentionDuration' = 'interval 7 days')\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                      |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|InvoiceNo                   |int                                                                                            |       |\n",
      "|StockCode                   |string                                                                                         |       |\n",
      "|Description                 |string                                                                                         |       |\n",
      "|Quantity                    |int                                                                                            |       |\n",
      "|InvoiceDate                 |string                                                                                         |       |\n",
      "|UnitPrice                   |double                                                                                         |       |\n",
      "|CustomerID                  |int                                                                                            |       |\n",
      "|Country                     |string                                                                                         |       |\n",
      "|                            |                                                                                               |       |\n",
      "|# Partitioning              |                                                                                               |       |\n",
      "|Not partitioned             |                                                                                               |       |\n",
      "|                            |                                                                                               |       |\n",
      "|# Detailed Table Information|                                                                                               |       |\n",
      "|Name                        |deltademo.salesdeltaformat                                                                     |       |\n",
      "|Location                    |file:/tmp/spark/data/delta/online_retail_data                                                  |       |\n",
      "|Provider                    |delta                                                                                          |       |\n",
      "|Table Properties            |[delta.deletedFileRetentionDuration=interval 7 days,delta.logRetentionDuration=interval 7 days]|       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify our changes\n",
    "spark.sql(\"describe extended SalesDeltaFormat\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted path /tmp/spark/data/delta/online_retail_data\n",
      "Deleted path /tmp/spark/data/delta/online_retail_data\n",
      "Deleted path /tmp/spark/data/parquet/online_retail_data\n"
     ]
    }
   ],
   "source": [
    "# Clean up from last run.\n",
    "# Clean up from last run.\n",
    "! rm -Rf $sourceData 2>/dev/null\n",
    "print(\"Deleted path \" + deltaPath)\n",
    "\n",
    "\n",
    "! rm -Rf $deltaPath 2>/dev/null\n",
    "print(\"Deleted path \" + deltaPath)\n",
    "\n",
    "! rm -Rf $parquetPath 2>/dev/null\n",
    "print(\"Deleted path \" + parquetPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
